{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jhUWkakza7OQ"
   },
   "source": [
    "# Sentiment Analysis with LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-dUnKhUba7OQ"
   },
   "source": [
    "In this notebook, we'll be looking at how to apply deep learning techniques to the task of sentiment analysis. Sentiment analysis can be thought of as the exercise of taking a sentence, paragraph, document, or any piece of natural language, and determining whether that text's emotional tone is positive, negative or neutral. \n",
    "\n",
    "This notebook will go through the general steps involved with all machine learning problems. Also, it will cover numerous topics like word vectors, recurrent neural networks, and long short-term memory units (LSTMs). **We will not be going into the specifics of ML algorithms in this notebook. The purpose of this notebook is to illustrate the typical steps in a ML pipeline that includes tasks outside of designing and developing a model.** So, don't get discouraged if you don't get a full understanding of what these terms mean at the first go. We will go over them at another time, and there will be links provided for additional reading on these topics.\n",
    "\n",
    "Before getting into the specifics, let's discuss the reasons why deep learning fits into natural language processing (NLP) tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jrnSYJbWa7OR"
   },
   "source": [
    "## Deep Learning for NLP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mm_CqC56a7OS"
   },
   "source": [
    "Natural language processing is all about creating systems that process or “understand” language in order to perform certain tasks. These tasks could include:\n",
    "\n",
    "* Question Answering - The main job of technologies like Siri, Alexa, and Cortana\n",
    "* Sentiment Analysis - Determining the emotional tone behind a piece of text\n",
    "* Image to Text Mappings - Generating a caption for an input image\n",
    "* Machine Translation - Translating a paragraph of text to another language\n",
    "* Speech Recognition - Having computers recognize spoken words\n",
    "\n",
    "In the pre-deep learning era, NLP was a thriving field that saw lots of different advancements. However, in all of the successes in the aforementioned tasks, one needed to do a lot of data transformations and feature engineering (e.g. stopwords, stemming, part-of-speech tags, etc.) and thus had to have a lot of domain knowledge in linguistics. Entire 4 year degrees are devoted to this field of study, as practitioners needed to be comfortable with terms like phonemes and morphemes. In the past few years, deep learning has seen incredible progress and has largely removed the requirement of strong domain knowledge. As a result of the lower barrier to entry, applications to NLP tasks have been one of the biggest areas of deep learning research. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YqOwqmrSa7OT"
   },
   "source": [
    "## Extract the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sXCUc4v6bGX2"
   },
   "source": [
    "Since we are using Google Colaboratory, we need to get the data from Github. In real life, data will not be so nicely curated and stored in an easy-to-access location. You will typically need to get data from different sources often in different formats.\n",
    "\n",
    "**(Note: ONLY PERFORM THIS STEP IF YOU ARE USING THE GOOGLE COLABORATORY VERIOSN OF THIS NOTEBOOK, and have not already cloned the Git repo that this notebook is part of. Also, you only need to do this once. The repo will persist on the GCE instance your notbook is attached to)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 102,
     "output_extras": [
      {
       "item_id": 5
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10727,
     "status": "ok",
     "timestamp": 1515092809799,
     "user": {
      "displayName": "Honto Ming",
      "photoUrl": "//lh4.googleusercontent.com/-rog1jLdpKWM/AAAAAAAAAAI/AAAAAAAAAAw/1O9kEY4kvJE/s50-c-k-no/photo.jpg",
      "userId": "116346329923336049999"
     },
     "user_tz": 480
    },
    "id": "-D9ka3B4bTcM",
    "outputId": "d3183228-7892-450e-85ed-4070c5a1f2d2"
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/honto-ming-hs/LSTM-Sentiment-Analysis.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xq9KDIk1jtRc"
   },
   "source": [
    "We then need to unzip the data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dDeP6h3ai9E0"
   },
   "outputs": [],
   "source": [
    "#!tar -xzf models.tar.gz\n",
    "#!tar -xzf training_data.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S5xDrM3la7OT"
   },
   "source": [
    "The dataset we're going to use is the Imdb movie review dataset. This set has 25,000 movie reviews, with 12,500 positive reviews and 12,500 negative reviews. Each of the reviews is stored in a txt file that we need to parse through. The positive reviews are stored in one directory and the negative reviews are stored in another. \n",
    "\n",
    "Since the data is relatively small in size in our example, we will read the reviews into memory as a Python list of Strings. In real life datasets are often much larger. If it can be fit into memory, we would use optimized data structures such as [Pandas](https://pandas.pydata.org/) dataframes. If it cannot be fit into memory, we may wish to take a random sample so that it does fit, or use distributed in-memory tools such as [Apache Spark](https://spark.apache.org) on a cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51,
     "output_extras": [
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2053,
     "status": "ok",
     "timestamp": 1515092848220,
     "user": {
      "displayName": "Honto Ming",
      "photoUrl": "//lh4.googleusercontent.com/-rog1jLdpKWM/AAAAAAAAAAI/AAAAAAAAAAw/1O9kEY4kvJE/s50-c-k-no/photo.jpg",
      "userId": "116346329923336049999"
     },
     "user_tz": 480
    },
    "id": "VFBhGhzQa7OU",
    "outputId": "c039b4de-4a71-4e53-989f-8a540e71b044"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "positiveFiles = ['positiveReviews/' + f for f in \n",
    "                 listdir('positiveReviews/') if \n",
    "                 isfile(join('positiveReviews/', f))]\n",
    "negativeFiles = ['negativeReviews/' + f for f in \n",
    "                 listdir('negativeReviews/') if \n",
    "                 isfile(join('negativeReviews/', f))]\n",
    "numWords = []\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kBVTqcama7Oc"
   },
   "source": [
    "## Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f2AQZ-V9a7Oc"
   },
   "source": [
    "The first step is to take a look at your data. The following code will show 5 random samples from each of the positive and negative reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "cXt8vijonMnD"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def print_rand_review(positive = True, num_to_print=5):\n",
    "  type_rev = \"positive\" if positive else \"negative\"\n",
    "  print(\"{} random {} reviews:\".format(num_to_print, type_rev))\n",
    "  for x in range(num_to_print):\n",
    "    with open(random.choice(positiveFiles), \"r\", encoding='utf-8') as f:\n",
    "      print(f.readline())\n",
    "      print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 224,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1515094183473,
     "user": {
      "displayName": "Honto Ming",
      "photoUrl": "//lh4.googleusercontent.com/-rog1jLdpKWM/AAAAAAAAAAI/AAAAAAAAAAw/1O9kEY4kvJE/s50-c-k-no/photo.jpg",
      "userId": "116346329923336049999"
     },
     "user_tz": 480
    },
    "id": "aH1tEiVXpREb",
    "outputId": "149baaf6-7f9f-49ae-a248-eda4a3201298"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 random positive reviews:\n",
      "This is one of the most daring and important of the so-called \"Pre-Code\" films made in Hollywood during the 1930s. Unlike some Pre-Code films that occasionally dabbled in subjects that would have never been allowed after 1934-5, this film fully immersed itself in a very sordid yet entertaining plot from start to finish. The conventional morality of the late 30s and 40s was definitely NOT evident in this film, as the film is essentially about a conniving woman who sleeps her way to the top--and with no apologies along the way. This \"broad\" both enjoyed sex and used it on every man who could help her get rich--something you just never would have seen in films made just two or three years later.<br /><br />The film begins with Barbara Stanwyck working in her father's speakeasy. In addition to being her boss, he is also her pimp and encourages her to sleep with a local government official so that he'll allow the illegal bar to operate with impunity. While not especially clear here, it appears as if Daddy has been \"renting\" his daughter's body out for a long time.<br /><br />However, after nearly being raped and attacking this man by breaking a bottle of beer over his skull, Barbara has had enough and heads to the big city. It doesn't hurt that the still blew up and killed her father, but her feeling that she was whoring herself out and had nothing to show for it appeared to be the impetus to move.<br /><br />Despite the Depression, Barbara uses sex to get a job at a huge mega-bank. She starts out at a pretty menial job as a file clerk, but in the space of what seems like just a few weeks, she sleeps her way from one job to another to yet another--until she is sleeping with the head of the bank and his future son-in-law!!! This all ends in tragedy, but Babs doesn't seem too shook up over the deaths of these two men. In fact, some time later, she is able to insinuate herself into the life of the NEW CEO and once again she's on top (perhaps in more way than one).<br /><br />Now so far, this is a wonderful movie because it was so gritty and unrepentant. Barbara played a 100% sociopath--a woman with no morality and no conscience--just a desire to squeeze as much out of life as she could no matter who she hurt in the process. However, the brave writers and producer \"chickened out\" and thought it important to tack on a redemptive ending. Considering that this woman was so evil and conniving, her change of heart at the end was a major disappointment and strongly detracted from the film. In many ways, this reminded me of the ending of JEZEBEL--as once again, a wicked person somehow \"sees the light\" and changes at the not-too-convincing conclusion.<br /><br />My advice is to try watching RED-HEADED WOMAN and DOWNSTAIRS. RED-HEADED WOMAN is much like BABY FACE but features no magical transformation at the end--the leading lady really is a skunk down deep! In DOWNSTAIRS, a film very much like RED-HEADED WOMAN, the roles are reversed and a man (John Gilbert) plays a similar conniving character. Both are classics and a bit better than this film.<br /><br />This film is an amazing curio of a brief period of often ultra-sleazy Hollywood films and in this light it's well worth seeing for Cinephiles. Also, fans of The Duke take note--John Wayne plays a very small part in the film and it's very unusual to see a very young Wayne playing such a conventional role.\n",
      "\n",
      "This is indeed a funny show, done in a creepy sort of way, much like a Tim Burton film. It's worth a look, as it's far more creative than most of the shows this season. Best of all, it's not a \"reality\" show. I'm wondering why the viewing public is so ready to accept shows like that (which lack creativity) and ignore wonderful shows like this that actually have a creative bent.<br /><br />While some decry the premise, I think it's really unusual. Much more enjoyable than \"Ghost Whisperer\" and \"Medium\". I think it's the funniest thing on the tube since \"My Name is Earl\".<br /><br />Oh, and the narration and music are wonderful. If you enjoy shows that are a bit off the beaten path, I'd recommend it. It's not as strange as Twin Peaks was, but it's got a serious kink to it.\n",
      "\n",
      "Not for people without swift mind or without a drop of Balkan blood in their veins. If You don't have any of these You can not understand it. And if you don't understand, you can't enjoy it. :) For example if you think Picasso is a name of a car produced by Citroen, probably if you see a Picasso's painting you just will walk by it, deciding that it's a trash-work of some street painter. :) So do not judge, before trying to understand it :) In the end i think it's a MUST for every one with open minds. Still my N1 remains The Shawshank Redemption! And remember that not all things can be put in frames. Because there are things in this world, that any frame just won't fit.\n",
      "\n",
      "The 74th Oscars was a very good one. Whoopi's work as EmCee was very funny, and light. I personally loved her last apperance, which garnered some frigid reviews due to coarse language and salacious jokes, but that's fine. The audience seemed to like it. Halle Berry, Denzel Washington, Ron Howard, Woody Allen, and Sidney Poitier made this an Oscar telecast to remember.<br /><br />\n",
      "\n",
      "Jake Speed (1986) was an amusing parody of Indiana Jones and other adventurer films that were popular during the eighties. Wayne Crawford stars as Jake Speed, an adventurer who's always battling evil doers wherever he goes. With his assistant Desmond Floyd (Dennis Christopher) they globe trot looking for some action (and some decent story lines). The duo meet a young woman named Margaret (Karen Kopins) who's sister has been kidnapped by an evil white slaver trader (John Hurt). Can she find and convince Jake and Desmond to help her rescue her sibling?<br /><br />A sappy and cheesy film that doesn't pretend to be something that it's not. I have to give this one a recommendation. That's if you enjoy movies that like to have fun and for those who don't take everything at face value.<br /><br />Recommended.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_rand_review()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 224,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1515094201199,
     "user": {
      "displayName": "Honto Ming",
      "photoUrl": "//lh4.googleusercontent.com/-rog1jLdpKWM/AAAAAAAAAAI/AAAAAAAAAAw/1O9kEY4kvJE/s50-c-k-no/photo.jpg",
      "userId": "116346329923336049999"
     },
     "user_tz": 480
    },
    "id": "PBU71OhaprTL",
    "outputId": "0a47e1dd-e8fc-49b3-c5f3-c628baae1c4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 random negative reviews:\n",
      "Fassbinder's most lavish production sacrifices little of his talent for identifying and deconstructing a locus of suffering in long, mobile takes that somehow also act as social encapsulations; here, it's much more overt, since the story takes place in war-torn Germany at the end of WWII, and the central character is a woman (Hanna Schygulla as Maria) who capitalizes on vulnerabilities (both economic and gender-related) to catapult herself up the ladder of a prominent textile corp. that makes coveted goods like lederhosen available to indigent workers (as she once was). Married amidst allied air raids, Maria and her new husband Herrmann are allowed a brief honeymoon before he's shipped out to the Russian front. In his absence, her despair is great: she spends most days at the train station, waiting for him to return. When he is reported dead, she abruptly stops grieving and takes a job as a barmaid/prostitute at a brothel catering to American GI's.<br /><br />When he returns, things get plenty messy, as circumstances (and his sense of noble self-sacrifice) conspire to keep them apart. The message is Fassbinder's M.O. writ large: 'Love is colder than death,' but not only is Maria contending with her own sanity and a husband largely incapable of loving her, but a country in deep flux with no discernible light at the end of the tunnel. Fassbinder is making some kind of statement on post-war Germany selling out to the highest bidder, but as with all his films, I tend to block those elements out and focus on the unbearable passions on display: Fassbinder's as evoked through his characters; his actors' as filtered through their real-life connections with Fassbinder. Taken together, his films can be either unbearable or indescribably mesmeric, often at once; this falls somewhere in-between, although definitely closer to the latter. While I didn't like it quite as much as The Bitter Tears of Petra von Kant or Katzelmacher, Maria Braun certainly has a greater scope and what's more, I could feel its passion and authentic detail to human emotions.\n",
      "\n",
      "In \"Brave New Girl,\" Holly comes from a small town in Texas, sings \"The Yellow Rose of Texas\" at a local competition, and gets admitted to a prestigious arts college in Philadelphia. From there the movie grows into a colorful story of friendship and loyalty. I loved this movie. It was full of great singing and acting and characters that kept it moving at a very nice pace. The acting was, of course, wonderful. Virginia Madsen and Lindsey Haun were outstanding, as well as Nick Roth The camera work was really done well and I was very pleased with the end (It seems a sequel could be in the making). Kudos to the director and all others that participated on this production. Quite a gem in the film archives.\n",
      "\n",
      "God Bless 80's slasher films. This is a fun, fun movie. This is what slasher films are all about. Now I'm not saying horror movies, just slasher films. It goes like this: A high school nerd is picked on by all these stupid jocks and cheerleaders, and then one of their pranks goes horribly wrong. Disfigured and back for revenge, sporting a Joker/Jester mask (pretty creepy looking, might i add), Marty begins to kill off those teens one by one many years later, after he manages to make them believe that their old abandoned high school is having a reunion. That is basically the plot? What's wrong with that? That's the beauty of 80's slasher films, most of them i would say. A lot of things could be so ridiculous, but they keep drawing you more in an' in as they go by. Especially this film.<br /><br />It features some outrageous killings, and some are quite creative as well. (poisoning of a beer can, acid bath, i can't remember a javelin ever being used before in any other slasher film either)It really is a fun, fun movie. That's all it is. Nevermind the fact that the characters are complete idiots, never mind their stupidity, and never mind the outrageous, random things that occur in this film. Such as lights being able to be controlled by the killer (when he's not even switching any buttons, you'll see) and toilets being able to cough up blood, baths being able to have acid come out of them, just use that as part of your entertainment! Because thats what really makes it entertaining.<br /><br />Movies like this represent 80's slashers. Never again could movies like this get made, know why? It isn't the 80's anymore. That is why you should just cherish them for what they are, good fun! I highly recommend this film if you're a hardcore fan of Slahsers such as Friday the 13th.<br /><br />One last note this movie also had a kick ass villain as well, Marty Rantzen. A disfigured, nerd, who kills all his old foes in a creepy Jester mask. A good villain makes a good slasher. Simon Scuddamore, who played Marty apparently committed suicide shortly after Slaughter High was released. That alone adds something creepy to the film, and sticks with it and it even makes you feel more sorry for the Marty character, i guess. All in all, great 80's slashers fun! It's a shame it will never be the same again...\n",
      "\n",
      "This movie is about human relationships. Charming, funny, and well written, with meaningful text. It seems that Morgan Freeman surely have fun at the set. Also good music. Paz Vega is a beautiful and smart woman. I really enjoy her acting. Woman like her are a good motivation to learn Spanish language. From the moment Morgan Freeman meets the cute Paz Vega the view is taken on an intimate journey with two strangers learning to care about where their lives are headed. 10 Items or Less is about zest of life. If you enjoy this film see also The Pursuit of Happiness with Will Smith and his son. Thats not a action film or a nude comedy. Its all about human relations.\n",
      "\n",
      "La Maman et la Putain has to be watched as a movie that is both related to the time it was released (post-68) and eternal in many respects. True, the actors don't \"act\" ... True, they talk a lot... But what they talk about is just what makes life worth living... or dying. The very long monologue spoken by Françoise Lebrun is perhaps the most accurate and moving text that was ever written about womanhood, manhood and love. Not easy to translate accurately, though. This movie is a statement about the difficulty of being a man and a woman (or two women in this case). And IMHO, Jean Pierre Léaud is one of the greatest French actors.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_rand_review(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lkzjSYCTnMzc"
   },
   "source": [
    "As you can see the reviews are quite long which is good because the longer the review, the more information it contains.\n",
    "\n",
    "As you can see the reviews are quite long which is bad because the longer the review, more we have to concern about the distance between words that have similar context.\n",
    "\n",
    "The following piece of code will determine total and average number of words in each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 259,
     "status": "ok",
     "timestamp": 1515094369067,
     "user": {
      "displayName": "Honto Ming",
      "photoUrl": "//lh4.googleusercontent.com/-rog1jLdpKWM/AAAAAAAAAAI/AAAAAAAAAAw/1O9kEY4kvJE/s50-c-k-no/photo.jpg",
      "userId": "116346329923336049999"
     },
     "user_tz": 480
    },
    "id": "el-ZsdL_a7Od",
    "outputId": "34186c07-b2e0-433e-f77e-340c01271890"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of files is 25000\n",
      "The total number of words in the files is 5844680\n",
      "The average number of words in the files is 233.7872\n"
     ]
    }
   ],
   "source": [
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_ETm_Q5a7Og"
   },
   "source": [
    "We can also use the Matplot library to visualize this data in a histogram format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 283,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 490,
     "status": "ok",
     "timestamp": 1515094371639,
     "user": {
      "displayName": "Honto Ming",
      "photoUrl": "//lh4.googleusercontent.com/-rog1jLdpKWM/AAAAAAAAAAI/AAAAAAAAAAw/1O9kEY4kvJE/s50-c-k-no/photo.jpg",
      "userId": "116346329923336049999"
     },
     "user_tz": 480
    },
    "id": "yLGjCDXza7Og",
    "outputId": "e87f5d4f-8075-4308-cee7-b867e7f662ff"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc40c06d080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numWords, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 1200, 0, 8000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b0FArhZna7Ok"
   },
   "source": [
    "From the histogram as well as the average number of words per file, we can safely say that most reviews will fall under 250 words, which is the max sequence length value we will set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "48ZVarGZa7Ol"
   },
   "outputs": [],
   "source": [
    "maxSeqLength = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7MHQw690a7Oo"
   },
   "source": [
    "There are other ways you can explore this data such as counting the number of unique words in your corpus, using regex to detect URLs and see how many messages contain URLs, and looking at the distribution of word occurences. For the purpose of this exercise, we will end our data exploration here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1y9Dtn43a7Op"
   },
   "source": [
    "## Transform the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N82oNd1fa7Op"
   },
   "source": [
    "Data is rarely in the format that is useful for a machine learning algorithm. In our case, the most obvious transformation requirement is turning the text in a form that _cannot_ be understood by ML algorithms (String) to a form that _can_: a numerical vector (aka word vectors/embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ReIDHPXWa7Oq"
   },
   "source": [
    "### Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zrdcnGeZa7Oq"
   },
   "source": [
    "In order to understand how deep learning can be applied, think about all the different forms of data that are used as inputs into machine learning or deep learning models. Computer vision ML models use arrays of pixel values, and logistic regression uses quantifiable features. The common theme is that the inputs need to be scalar values, or matrices of scalar values. When you think of NLP tasks, however, a data pipeline like this may come to mind. \n",
    " \n",
    "![caption](https://github.com/honto-ming-hs/LSTM-Sentiment-Analysis/blob/master/Images/SentimentAnalysis.png?raw=true)\n",
    "\n",
    "This kind of pipeline is problematic. There is no way for us to do common operations in a neural network like dot products or backpropagation on a single string. Instead of having a string input, we will need to convert each word in the sentence to a vector. \n",
    "\n",
    "![caption](https://github.com/honto-ming-hs/LSTM-Sentiment-Analysis/blob/master/Images/SentimentAnalysis2.png?raw=true)\n",
    "\n",
    "You can think of the input to the sentiment analysis module as being a 16 x D dimensional matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RROUzRkda7Or"
   },
   "source": [
    "We want these vectors to be created in such a way that they somehow represent the word and its context, meaning, and semantics. For example, we’d like the vectors for the words “love” and “adore” to reside in relatively the same area in the vector space since they both have similar definitions and are both used in similar contexts. The vector representation of a word is also known as a word embedding.\n",
    "\n",
    "![caption](https://github.com/honto-ming-hs/LSTM-Sentiment-Analysis/blob/master/Images/SentimentAnalysis8.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w0moVVQM3cUg"
   },
   "source": [
    "The next subsection goes into a litle mroe detail _how_ word vectors are trained and how it encapsulates the semanticl and/or contextual meaning of a word in a numerical vector. We will skip this sub-section for now but read on if you're interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "26kvR4qoa7Os"
   },
   "source": [
    "#### Optional: Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8qw_XPkVa7Os"
   },
   "source": [
    "In order to create these word embeddings, we'll use an *additional* model that's commonly refered to as \"Word2Vec\". Without going into too much detail, the model creates word vectors by looking at the context with which words appear in sentences. Words with similar contexts will be placed close together in the vector space. In natural language, the context of words can be very important when trying to determine their meanings. Taking our previous example of the words \"adore\" and \"love\", consider the types of sentences we'd find these words in.\n",
    "\n",
    "![caption](https://github.com/honto-ming-hs/LSTM-Sentiment-Analysis/blob/master/Images/SentimentAnalysis9.png?raw=true)\n",
    "\n",
    "From the context of the sentences, we can see that both words are generally used in sentences with positive connotations and generally precede nouns or noun phrases. This is an indication that both words have something in common and can possibly be synonyms. Context is also very important when considering grammatical structure in sentences. Most sentences will follow traditional paradigms of having verbs follow nouns, adjectives precede nouns, and so on. For this reason, the model is more likely to position nouns in the same general area as other nouns. The model takes in a large dataset of sentences (English Wikipedia for example) and outputs vectors for each unique word in the corpus. The output of a Word2Vec model is called an embedding matrix.\n",
    "\n",
    "![caption](https://github.com/honto-ming-hs/LSTM-Sentiment-Analysis/blob/master/Images/SentimentAnalysis3.png?raw=true)\n",
    "\n",
    "This embedding matrix will contain vectors for every distinct word in the training corpus. Traditionally, embedding matrices can contain over 3 million word vectors.\n",
    "\n",
    "The Word2Vec model is trained by taking each sentence in the dataset, sliding a window of fixed size over it, and trying to predict the center word of the window, given the other words. Using a loss function and optimization procedure, the model generates vectors for each unique word. The specifics of this training procedure can get a little complicated, so we’re going to skip over the details for now, but the main takeaway here is that inputs into any Deep Learning approach to an NLP task will likely have word vectors as input.\n",
    "\n",
    "For more information on the theory behind Word2Vec and how you create your own embeddings, check out Tensorflow's [tutorial](https://www.tensorflow.org/tutorials/word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PLcrzXgJ3mhk"
   },
   "source": [
    "### Using Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kPkYwEAIuuAg"
   },
   "source": [
    "If we had a very large corpus of movie reviews, we can train our own Word2Vec model for custom word vectors that will work very well with movie reviews but may not work well with other text. But others with access to large corpuses have done similar things before so for simplicity, we're going to be using a pretrained model. \n",
    "\n",
    "As one of the biggest players in the ML game, Google was able to train a Word2Vec model on a massive Google News dataset that contained over 100 billion different words! From that model, Google [was able to create 3 million word vectors](https://code.google.com/archive/p/word2vec/#Pre-trained_word_and_phrase_vectors), each with a dimensionality of 300. \n",
    "\n",
    "In an ideal scenario, we'd use those vectors, but since the word vectors matrix is quite large (3.6 GB!), we'll be using a much more manageable matrix that is trained using [GloVe](http://nlp.stanford.edu/projects/glove/), a similar word vector generation model. The matrix will contain 400,000 word vectors, each with a dimensionality of 50. \n",
    "\n",
    "We're going to be importing two different data structures, one will be a Python list with the 400,000 words, and one will be a 400,000 x 50 dimensional embedding matrix that holds all of the word vector values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51,
     "output_extras": [
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 510,
     "status": "ok",
     "timestamp": 1515095659549,
     "user": {
      "displayName": "Honto Ming",
      "photoUrl": "//lh4.googleusercontent.com/-rog1jLdpKWM/AAAAAAAAAAI/AAAAAAAAAAw/1O9kEY4kvJE/s50-c-k-no/photo.jpg",
      "userId": "116346329923336049999"
     },
     "user_tz": 480
    },
    "id": "lhG8raTYvL_H",
    "outputId": "8ecb7ac6-8430-4fbd-81e1-b75e5f876ba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the word list!\n",
      "Loaded the word vectors!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "wordsList = np.load('wordsList.npy')\n",
    "print('Loaded the word list!')\n",
    "wordsList = wordsList.tolist() #Originally loaded as numpy array\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load('wordVectors.npy')\n",
    "print ('Loaded the word vectors!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZBbXP_8evT1L"
   },
   "source": [
    "Just to make sure everything has been loaded in correctly, we can look at the dimensions of the vocabulary list and the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Wgj3JVu8vW7N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "(400000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(len(wordsList))\n",
    "print(wordVectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VPMC_f7AvYo2"
   },
   "source": [
    "We can also search our word list for a word like \"baseball\", and then access its corresponding vector through the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 187,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1515095704816,
     "user": {
      "displayName": "Honto Ming",
      "photoUrl": "//lh4.googleusercontent.com/-rog1jLdpKWM/AAAAAAAAAAI/AAAAAAAAAAw/1O9kEY4kvJE/s50-c-k-no/photo.jpg",
      "userId": "116346329923336049999"
     },
     "user_tz": 480
    },
    "id": "9W3xyW4nvan4",
    "outputId": "eb8ebe6a-0744-40d9-bc73-e35eac130bf6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.93270004,  1.04209995, -0.78514999,  0.91033   ,  0.22711   ,\n",
       "       -0.62158   , -1.64929998,  0.07686   , -0.58679998,  0.058831  ,\n",
       "        0.35628   ,  0.68915999, -0.50598001,  0.70472997,  1.26639998,\n",
       "       -0.40031001, -0.020687  ,  0.80862999, -0.90565997, -0.074054  ,\n",
       "       -0.87674999, -0.62910002, -0.12684999,  0.11524   , -0.55685002,\n",
       "       -1.68260002, -0.26291001,  0.22632   ,  0.713     , -1.08280003,\n",
       "        2.12310004,  0.49869001,  0.066711  , -0.48225999, -0.17896999,\n",
       "        0.47699001,  0.16384   ,  0.16537   , -0.11506   , -0.15962   ,\n",
       "       -0.94926   , -0.42833   , -0.59456998,  1.35660005, -0.27506   ,\n",
       "        0.19918001, -0.36008   ,  0.55667001, -0.70314997,  0.17157   ], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseballIndex = wordsList.index('baseball')\n",
    "wordVectors[baseballIndex]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t8FOdEF6vnuL"
   },
   "source": [
    "Now that we have our vectors, our first step is taking an input sentence and then constructing the its vector representation. Recall that we capped the length of each review at 250 words. If we convert each word directly into it's 50-D vector representation (as shoen in the output of the cell above), each review will be represented as a 250 x 50 matrix. However, this isn't the most efficient use of memory since we're keeping duplicates of a word vector every time the word occurs in our corpus. Luckily, Tensorflow (and many other deep learning frameworks) realizes this and provides a built-in embedding lookup function so that we can transform each word into the *index* of the word in the embedding matrix so that we do a lazy lookup at time of computation.\n",
    "\n",
    "Let's say that we have the input sentence \"I thought the movie was incredible and inspiring\". In order to get the word vectors, we will use Tensorflow's embedding lookup function. This function takes in two arguments, one for the embedding matrix (the wordVectors matrix in our case), and one for the ids of each of the words. The ids vector can be thought of as the integerized representation of the training set. This is basically just the row index of each of the words. Let's look at a quick example to make this concrete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 545,
     "status": "ok",
     "timestamp": 1515103145745,
     "user": {
      "displayName": "Honto Ming",
      "photoUrl": "//lh4.googleusercontent.com/-rog1jLdpKWM/AAAAAAAAAAI/AAAAAAAAAAw/1O9kEY4kvJE/s50-c-k-no/photo.jpg",
      "userId": "116346329923336049999"
     },
     "user_tz": 480
    },
    "id": "H_z5Vlv1w_L4",
    "outputId": "8b9000e9-ffbc-4bb5-f1fd-0c3ad4b41299"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tf_py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[    41    804 201534   1005     15   7446      5  13767      0      0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "seqLength = 10 #Maximum length of sentence\n",
    "numDimensions = 300 #Dimensions for each word vector\n",
    "firstSentence = np.zeros((seqLength), dtype='int32')\n",
    "firstSentence[0] = wordsList.index(\"i\")\n",
    "firstSentence[1] = wordsList.index(\"thought\")\n",
    "firstSentence[2] = wordsList.index(\"the\")\n",
    "firstSentence[3] = wordsList.index(\"movie\")\n",
    "firstSentence[4] = wordsList.index(\"was\")\n",
    "firstSentence[5] = wordsList.index(\"incredible\")\n",
    "firstSentence[6] = wordsList.index(\"and\")\n",
    "firstSentence[7] = wordsList.index(\"inspiring\")\n",
    "#firstSentence[8] and firstSentence[9] are going to be 0\n",
    "print(firstSentence.shape)\n",
    "print(firstSentence) #Shows the row index for each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KRcfz-e3w8x1"
   },
   "source": [
    "The data pipeline can be illustrated below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UL1jhkX-a7O_"
   },
   "source": [
    "![caption](https://github.com/honto-ming-hs/LSTM-Sentiment-Analysis/blob/master/Images/SentimentAnalysis5.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Mh2dWKkBYWZ"
   },
   "source": [
    "The 10 x 50 output should contain the 50 dimensional word vectors for each of the 10 words in the sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 735,
     "status": "ok",
     "timestamp": 1515100443585,
     "user": {
      "displayName": "Honto Ming",
      "photoUrl": "//lh4.googleusercontent.com/-rog1jLdpKWM/AAAAAAAAAAI/AAAAAAAAAAw/1O9kEY4kvJE/s50-c-k-no/photo.jpg",
      "userId": "116346329923336049999"
     },
     "user_tz": 480
    },
    "id": "3R4bblMyBZkk",
    "outputId": "51a56e2e-f224-4b3f-d2c5-6fed2eb99dea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors,firstSentence).eval().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pQTc-k5-CIg5"
   },
   "source": [
    "We're not quite there yet. Notice that the reviews are written in normal mixed case, and it contains punctuation and some html tags such as ``<br />``.\n",
    "\n",
    "If you then look at the 400,000 word vocabulary covered by the pre-trained GloVe embeddings you will notice that all words are in lowercase, it contains some puncuation, and it contains no HTML tags. We will need to make transformations to the text before we can do lookups in the embedding matrix. \n",
    "\n",
    "The following code will do help us do the following transformations:\n",
    "\n",
    "* Lowercase all chars\n",
    "* Keep only alphanumeric characters r\n",
    "* Remove ``<br />``'s'. \n",
    "\n",
    "There are additional transformations we _can_ take (e.g. parsing by whitespace and punctuation \"this. is. \" -> \"this . is . \") but this will suffice for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "xEmcuanvGqlg"
   },
   "outputs": [],
   "source": [
    "# Removes punctuation, parentheses, question marks, etc., and leaves only \n",
    "# alphanumeric characters\n",
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 561,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1515103161709,
     "user": {
      "displayName": "Honto Ming",
      "photoUrl": "//lh4.googleusercontent.com/-rog1jLdpKWM/AAAAAAAAAAI/AAAAAAAAAAw/1O9kEY4kvJE/s50-c-k-no/photo.jpg",
      "userId": "116346329923336049999"
     },
     "user_tz": 480
    },
    "id": "YidntHswKGmG",
    "outputId": "0a27e935-40b3-4a3c-c05a-ccac2d5715ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    21,    345, 399999,   1702,     12,     37,     14,     36,\n",
       "            7,    143,    116,   1005,     34,     47,      7,    191,\n",
       "          219,   1062,      3, 201534,    567,    770,      5, 201534,\n",
       "          254,  15884,    100, 399999,     83,     81,     32,      7,\n",
       "        72574,  23635,    127,     37,     14,      7,   4320,     10,\n",
       "       399999,     63,     32,     87,     66,      3,    133,  85057,\n",
       "          235,    697,   3354,      6,     37,   1005,     47,   2248,\n",
       "         2050,     22,    254,     34,    127,     37,     14,    149,\n",
       "           19,     16,    106,      7,   3922,   3830,    285,   4674,\n",
       "          913,      7,   2357,    605,   3267,      3,    162,    573,\n",
       "       399999,   6163,      3, 201534,    126,     85, 399999,     63,\n",
       "          189,     30,   1150,      3,    158,   2459,   1898,     95,\n",
       "          102,     39,    119,    100,     20,  58544,    169,   7063,\n",
       "           21,    182, 399999,  11202,   1813, 136283,   1465, 399999,\n",
       "         1716,   2459,    100,     83,     81,    303,      7,    608,\n",
       "       399999,    672, 399999,     14, 201534,   2745,     59,   9828,\n",
       "       399999,  13616,      5, 201534,     43,      4,    838,     61,\n",
       "           64,   1348,    402,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstFile = np.zeros((maxSeqLength), dtype='int32')\n",
    "with open(random.choice(positiveFiles)) as f:\n",
    "    indexCounter = 0\n",
    "    line=f.readline()\n",
    "    cleanedLine = cleanSentences(line)\n",
    "    split = cleanedLine.split()\n",
    "    for word in split:\n",
    "        try:\n",
    "            firstFile[indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            firstFile[indexCounter] = 399999 #Index of vector for unknown words\n",
    "        indexCounter = indexCounter + 1\n",
    "firstFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y48h14lGL8k4"
   },
   "source": [
    "Notice that we are padding messages shorter than 250 words with 0, and that all out of vocabulary (OOV) words (i.e. all words in our reviews not found in the 400,000 word covered by the GloVe embeddings) have value 399999. It also may be smart here to look and see how many OOV words you have in your data. If there are too many OOV words you may need to use other pre-trained word vectors or train your own. We are going to make the assumption here that there are not too many OOV words in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FGGPM7tTMwfH"
   },
   "source": [
    "Now, let's convert our 25,000 reviews from strings to indices. We'll load in the movie training set and integerize it to get a 25000 x 250 matrix. This was a computationally expensive process, so you have the option of running the whole piece or load in a pre-computed IDs matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0oLuMbvgNIH1"
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\"\"\"RUN THIS CELL IF YOU WANT TO COMPUTE THE INTEGER INDICES\"\"\"\n",
    "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "fileCounter = 0\n",
    "for pf in positiveFiles:\n",
    "   with open(pf, \"r\") as f:\n",
    "       indexCounter = 0\n",
    "       line=f.readline()\n",
    "       cleanedLine = cleanSentences(line)\n",
    "       split = cleanedLine.split()\n",
    "       for word in split:\n",
    "           try:\n",
    "               ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "           except ValueError:\n",
    "               ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "           indexCounter = indexCounter + 1\n",
    "           if indexCounter >= maxSeqLength:\n",
    "               break\n",
    "       fileCounter = fileCounter + 1 \n",
    "\n",
    "for nf in negativeFiles:\n",
    "   with open(nf, \"r\") as f:\n",
    "       indexCounter = 0\n",
    "       line=f.readline()\n",
    "       cleanedLine = cleanSentences(line)\n",
    "       split = cleanedLine.split()\n",
    "       for word in split:\n",
    "           try:\n",
    "               ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "           except ValueError:\n",
    "               ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "           indexCounter = indexCounter + 1\n",
    "           if indexCounter >= maxSeqLength:\n",
    "               break\n",
    "       fileCounter = fileCounter + 1 \n",
    "#Pass into embedding function and see if it evaluates. \n",
    "\n",
    "np.save('idsMatrix', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "vU7GglTFOwEJ"
   },
   "outputs": [],
   "source": [
    "\"\"\"RUN THIS CELL IF YOU WANT TO LOAD THE MATRIX\"\"\"\n",
    "ids = np.load('idsMatrix.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPAeOL3BRRyo"
   },
   "source": [
    "### Training, Validation, and Testing Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHHD5HqDRX_j"
   },
   "source": [
    "For all machine learning problems, you will need to separate the data into training, validation, and testing sets. Below is a brief description of the purpose of each:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mW3xns9dXAcJ"
   },
   "source": [
    "**Training**\n",
    "* Used to train the model\n",
    "* Randomly sampled into mini batches so that the model training can be done iteratively for each mini batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KcCSkeoHXAfa"
   },
   "source": [
    "**Validation**\n",
    "* Used periodically through training (end of multipel min-batched) to spot for problems such as overfitting \n",
    "* Also used for tuning hyper-parameters and selection between different model architectures\n",
    "* Cant hink of it as a quasi test set\n",
    "* Can also be used for training if you do not have enough training data (Google \"cross-validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1YAQHljMXAkR"
   },
   "source": [
    "**Testing**\n",
    "* **NOT** used for training\n",
    "* Used at the _**very end of all training when you have selected the model to put into production**_ to estimate how well the model will perform in the real world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H5EunsrcS-TB"
   },
   "source": [
    "There are conflicting views on the purpose and use of a testing set. From a purist's perspective, the testing set should be used once and only once to report the accuracy of the model you have chosen to put into production. If there are multiple models being experimented with, you should compare their accuracy levels with the _validation_ set. Once you have used your testign set to compare the accuracy of multiple models, you are effectively changing it into a validation set. The reason behind this is to avoid overfitting. We will not go into the details here so just remember to set aside data for testing and do not use it until you ahve chosen which model to put into production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A5WtcTLmXRWq"
   },
   "source": [
    "There are many ways we can split the data into the aforementioned subsets. For our example, we will be assuming that the reviews are randomly sampled and ordered in their subdirectories (``LSTM-Sentiment-Analysis/positiveReviews/`` and ``LSTM-Sentiment-Analysis/negativeReviews/``). There are 12,500 reviews each for positive and negative respectivley, so for each type of review we will be taking 10500 for training, 1000 for validation, and 1000 for testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt_text](https://github.com/honto-ming-hs/LSTM-Sentiment-Analysis/blob/master/Images/train_test_split.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mf0ZU38OZhRA"
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getTrainBatch(batchSize):\n",
    "  labels = []\n",
    "  arr = np.zeros([batchSize, maxSeqLength])\n",
    "  for i in range(batchSize):\n",
    "      if (i % 2 == 0): \n",
    "          num = randint(1,10500)\n",
    "          labels.append([1,0])\n",
    "      else:\n",
    "          num = randint(14501,25000)\n",
    "          labels.append([0,1])\n",
    "      arr[i] = ids[num-1:num]\n",
    "  return arr, np.array(labels)\n",
    "\n",
    "\n",
    "def getTestSet():\n",
    "  # truth labels\n",
    "  labels = [[1,0]] * 1000\n",
    "  neg_labels = [[0,1]] * 1000\n",
    "  labels.extend(neg_labels)\n",
    "  # features (the middle 2000)\n",
    "  features = ids[11500:13500]\n",
    "  return np.array(features), np.array(labels)\n",
    "\n",
    "\n",
    "def getValidationSet():\n",
    "  # truth labels\n",
    "  labels = [[1,0]] * 1000\n",
    "  neg_labels = [[0,1]] * 1000\n",
    "  labels.extend(neg_labels)\n",
    "  # features\n",
    "  pos_features = ids[10500:11500]\n",
    "  neg_features = ids[13500:14500]\n",
    "  return np.vstack((pos_features, neg_features)), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5e0ZQmLj1CUf"
   },
   "source": [
    "Let's see what each of the above function returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QK05ccge1Hpd"
   },
   "source": [
    "Training set mini-batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "2XG_HBitmyaH"
   },
   "outputs": [],
   "source": [
    "batchSize = 5\n",
    "feat, lbl = getTrainBatch(batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1515113326871,
     "user": {
      "displayName": "Honto Ming",
      "photoUrl": "//lh4.googleusercontent.com/-rog1jLdpKWM/AAAAAAAAAAI/AAAAAAAAAAw/1O9kEY4kvJE/s50-c-k-no/photo.jpg",
      "userId": "116346329923336049999"
     },
     "user_tz": 480
    },
    "id": "3CnpLQJ6m-N0",
    "outputId": "a80b3b78-96d9-49a3-f365-c9af2c6e4421"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 250)\n",
      "(5, 2)\n"
     ]
    }
   ],
   "source": [
    "print(feat.shape)\n",
    "print(lbl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dn_E5Exy1Kbh"
   },
   "source": [
    "The whole test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "u-SlfMNWnN6J"
   },
   "outputs": [],
   "source": [
    "feat, lbl = getTestSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1515113562928,
     "user": {
      "displayName": "Honto Ming",
      "photoUrl": "//lh4.googleusercontent.com/-rog1jLdpKWM/AAAAAAAAAAI/AAAAAAAAAAw/1O9kEY4kvJE/s50-c-k-no/photo.jpg",
      "userId": "116346329923336049999"
     },
     "user_tz": 480
    },
    "id": "AE1Wubd2tXCT",
    "outputId": "694a8f49-6979-4c81-d526-b6ce1b987ec8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 250)\n",
      "(2000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(feat.shape)\n",
    "print(lbl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kqOVwGnM1SGm"
   },
   "source": [
    "The whole validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ZxvhZepOuIsC"
   },
   "outputs": [],
   "source": [
    "feat, lbl = getValidationSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1515113801803,
     "user": {
      "displayName": "Honto Ming",
      "photoUrl": "//lh4.googleusercontent.com/-rog1jLdpKWM/AAAAAAAAAAI/AAAAAAAAAAw/1O9kEY4kvJE/s50-c-k-no/photo.jpg",
      "userId": "116346329923336049999"
     },
     "user_tz": 480
    },
    "id": "Hs3T1_Khuwr0",
    "outputId": "98e8e1a4-3a76-4aa9-eb66-3af35b08453b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 250)\n",
      "(2000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(feat.shape)\n",
    "print(lbl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JTaaZTTT3Q18"
   },
   "source": [
    "## Build, Train, Tune, and Test Model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HjHvKajX1ySf"
   },
   "source": [
    "Now that we have the data transformed and ready for a machine learning model to train on, we can finally get to the machine learning part of this tutorial.  The model we are going to use here is a recurrent neural network (RNN), which is a type of neural networks (NN). Due to time limitations, we will not delve any deeper into the inner workings of NNs or RNNs. For the time being assume they are magical black boxes that will learn from data. If you're interested in learnign more: \n",
    "\n",
    "* A good e-book into Neural Networks in general can be found here: (http://neuralnetworksanddeeplearning.com/)\n",
    "* A good MOOC for Neural Networks in general can be found here: (http://www.fast.ai)\n",
    "\n",
    "The next series of cells under the headings **Recurrent Neural Networks (RNN)** and **Long Short Term Memory Units (LSTM)** goes into a bit more detail about RNNs and LSTMs. I have included them since it was part of the original O'Rielly tutorial. We will skip over these sections. For the time being, just think of RNNs as a flavour of the magical neural network black boxes that is suited for time series or sequence data, and LSTMs as a type of component within it. Why do we want to use RNN's for our problem? For sentiment analysis with movie reviews, RNNs will not only look at the single word in a review, but it will also consider the words that appeared earlier in the review; this is similar to how as humans we read an comprehend text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CrVtYyAda7Ot"
   },
   "source": [
    "### Recurrent Neural Networks (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vK-__QRca7Ot"
   },
   "source": [
    "Now that we have our word vectors as input, let's look at the actual network architecture we're going to be building. The unique aspect of NLP data is that there is a temporal aspect to it. Each word in a sentence depends greatly on what came before and comes after it. In order to account for this dependency, we use a recurrent neural network.  \n",
    "\n",
    "The recurrent neural network structure is a little different from the traditional feedforward NN you may be accostumed to seeing. The feedforward network consists of input nodes, hidden units, and output nodes. \n",
    "\n",
    "![caption](https://github.com/honto-ming-hs/LSTM-Sentiment-Analysis/blob/master/Images/SentimentAnalysis17.png?raw=true)\n",
    "\n",
    "The main difference between feedforward neural networks and recurrent ones is the temporal aspect of the latter. In RNNs, each word in an input sequence will be associated with a specific time step. In effect, the number of time steps will be equal to the max sequence length. \n",
    "\n",
    "![caption](https://github.com/honto-ming-hs/LSTM-Sentiment-Analysis/blob/master/Images/SentimentAnalysis18.png?raw=true)\n",
    "\n",
    "Associated with each time step is also a new component called a hidden state vector h<sub>t</sub>. From a high level, this vector seeks to encapsulate and summarize all of the information that was seen in the previous time steps. Just like x<sub>t</sub> is a vector that encapsulates all the information of a specific word, h<sub>t</sub> is a vector that summarizes information from previous time steps.\n",
    "\n",
    "The hidden state is a function of both the current word vector and the hidden state vector at the previous time step. The sigma indicates that the sum of the two terms will be put through an activation function (normally a sigmoid or tanh).\n",
    "\n",
    "![caption](https://github.com/honto-ming-hs/LSTM-Sentiment-Analysis/blob/master/Images/SentimentAnalysis15.png?raw=true)\n",
    "\n",
    "The 2 W terms in the above formulation represent weight matrices. If you take a close look at the superscripts, you’ll see that there’s a weight matrix W<sup>X</sup> which we’re going to multiply with our input, and there’s a recurrent weight matrix W<sup>H</sup> which is multiplied with the hidden state vector at the previous time step. W<sup>H</sup> is a matrix that stays the same across all time steps, and the weight matrix W<sup>X</sup> is different for each input. \n",
    "\n",
    "The magnitude of these weight matrices impact the amount the hidden state vector is affected by either the current vector or the previous hidden state. As an exercise, take a look at the above formula, and consider how h<sub>t</sub> would change if either W<sup>X</sup> or W<sup>H</sup> had large or small values. \n",
    "\n",
    "Let's look at a quick example. When the magnitude of W<sup>H</sup> is large and the magnitude of W<sup>X</sup> is small, we know that h<sub>t</sub> is largely affected by h<sub>t-1</sub> and unaffected by x<sub>t</sub>. In other words, the current hidden state vector sees that the current word is largely inconsequential to the overall summary of the sentence, and thus it will take on mostly the same value as the vector at the previous time step. \n",
    "\n",
    "The weight matrices are updated through an optimization process called backpropagation through time. \n",
    "\n",
    "The hidden state vector at the final time step is fed into a binary softmax classifier where it is multiplied by another weight matrix and put through a softmax function that outputs values between 0 and 1, effectively giving us the probabilities of positive and negative sentiment. \n",
    "\n",
    "![caption](https://github.com/honto-ming-hs/LSTM-Sentiment-Analysis/blob/master/Images/SentimentAnalysis16.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ISO3xdcNa7Ou"
   },
   "source": [
    "### Long Short Term Memory Units (LSTMs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0UYu-28La7Ov"
   },
   "source": [
    "Long Short Term Memory Units are modules that you can place inside of reucrrent neural entworks. At a high level, they make sure that the hidden state vector h is able to encapsulate information about long term dependencies in the text. As we saw in the previous section, the formulation for h in traditional RNNs is relatively simple. This approach won't be able to effectively connect together information that is separated by more than a couple time steps. We can illiustrate this idea of handling long term dependencies through an example in the field of question answering. The function of question answering models is to take an a passage of text, and answer a question about its content. Let's look at the following example.\n",
    "\n",
    "![caption](https://github.com/honto-ming-hs/LSTM-Sentiment-Analysis/blob/master/Images/SentimentAnalysis4.png?raw=true)\n",
    "\n",
    "Here, we see that the middle sentence had no impact on the question that was asked. However, there is a strong connection between the first and third sentences. With a classic RNN, the hidden state vector at the end of the network might have stored more information about the dog sentence than about the first sentence about the number. Basically, the addition of LSTM units make it possible to determine the correct and useful information that needs to be stored in the hidden state vector.\n",
    "\n",
    "Looking at LSTM units from a more technical viewpoint, the units take in the current word vector x<sub>t</sub> and output the hidden state vector h<sub>t</sub>. In these units, the formulation for h<sub>t</sub> will be a bit more complex than that in a typical RNN. The computation is broken up into 4 components, an input gate, a forget gate, an output gate, and a new memory container. \n",
    "\n",
    "![caption](https://github.com/honto-ming-hs/LSTM-Sentiment-Analysis/blob/master/Images/SentimentAnalysis10.png?raw=true)\n",
    "\n",
    "Each gate will take in x<sub>t</sub> and h<sub>t-1</sub> (not shown in image) as inputs and will perform some computation on them to obtain intermediate states. Each intermediate state gets fed into different pipelines and eventually the information is aggregated to form h<sub>t</sub>. For simplicity sake, we won't go into the specific formulations for each gate, but it's worth noting that each of these gates can be thought of as different modules within the LSTM that each have different functions. The input gate determines how much emphasis to put on each of the inputs, the forget gate determines the information that we'll throw away, and the output gate determines the final h<sub>t</sub> based on the intermediate states. For more information on understanding the functions of the different gates and the full equations, check out Christopher Olah's great [blog post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "Looking back at the first example with question “What is the sum of the two numbers?”, the model would have to be trained on similar types of questions and answers. The LSTM units would then be able to realize that any sentence without numbers will likely not have an impact on the answer to the question, and thus the unit will be able to utilize its forget gate to discard the unnecessary information about the dog, and rather keep the information regarding the numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bc8oYcr-a7Ow"
   },
   "source": [
    "### Building the RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zlzAzknea7Ow"
   },
   "source": [
    "As a quick review, so far we have done the following:\n",
    "\n",
    "    1) Training a word vector generation model (such as Word2Vec) or loading pretrained word vectors (GloVe). The result is a large matrix with each row reprresenting a word in our vocab\n",
    "    2) Transforming each review from String to a list of IDs that refers to the row index of the word in the pre-trained word vector matrix\n",
    "    3) Split the data set into training mini batches, validation set, and testing set\n",
    "    \n",
    "We will now build the machine learning model that will utilize our work from above. Building the machine learning model involves:\n",
    "\n",
    "    1) Define the Tensorflow graph for the RNN with LSTM units (fancy way of saying build the model)\n",
    "    2) Define and iteratively execute the training step (fancy way of saying train the model)\n",
    "    3) Validate with the validation set once every x training steps (fancy way of saying tune the model)\n",
    "    4) Calculate testing accuracy (fancy way fo saying test the model)\n",
    "    \n",
    "For our crash course, we will not have time to go into details about Tensorflow. Remember, the purpose of the crash course is to show the typical seps in a ML pipeline. From a ML pipeline perspective, you can replace Tensorflow with any other ML libraries and RNNs with any other type of ML model. Hence, for the time being you can treat the RNN Model described here as a black box that takes a list of word ID's and the corresponding positive/negative label as inputs, learns from it, and generates a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the Tensorflow Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "su4vjfvVa7PY"
   },
   "source": [
    "Now, we’re ready to start creating our Tensorflow graph. We’ll first need to define some hyperparameters, such as batch size, number of LSTM units, number of output classes, number of training iterations, and the dimensions fo our word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RJwrlFT1a7PY"
   },
   "outputs": [],
   "source": [
    "batchSize = 96 # how big our training mini-batches are per iteration\n",
    "lstmUnits = 64 # how complex our RNN is\n",
    "numClasses = 2 # number of classes in our prediction (2: positive, negative)\n",
    "iterations = 30000 # how many trainingiterations\n",
    "\n",
    "# specs for the embeddings\n",
    "doc_vocab_size = wordVectors.shape[0]\n",
    "embedding_dim = wordVectors.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zD2hxTu-a7PZ"
   },
   "source": [
    "As with most Tensorflow graphs, we’ll now need to specify two placeholders, one for the inputs into the network, and one for the labels. Placeholders are used in Tensorflow as placeholders for data that will be fed to Tensorflow when the graph is executed. The most important part about defining these placeholders is understanding each of their dimensionalities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hIWCHJs5a7Pa"
   },
   "source": [
    "The labels placeholder represents a set of values, each either [1, 0] or [0, 1], depending on whether each training example is positive or negative. Each row in the integerized input placeholder represents the integerized representation of each training example that we include in our batch.\n",
    "\n",
    "Note that the following blue blocks \"Integerized Inputs\" and \"Embedding Matrix\" and the green block \"Labels\" should have z-axis value of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OjUR5XDua7Pb"
   },
   "source": [
    "![caption](https://github.com/honto-ming-hs/LSTM-Sentiment-Analysis/blob/master/Images/SentimentAnalysis12.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 215,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 300,
     "status": "error",
     "timestamp": 1515115713269,
     "user": {
      "displayName": "Honto Ming",
      "photoUrl": "//lh4.googleusercontent.com/-rog1jLdpKWM/AAAAAAAAAAI/AAAAAAAAAAw/1O9kEY4kvJE/s50-c-k-no/photo.jpg",
      "userId": "116346329923336049999"
     },
     "user_tz": 480
    },
    "id": "bx1RL1yva7Pb",
    "outputId": "68bc934b-43dd-40cd-cf0b-3688136f4fa9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "# using None for the first dim so that we can accomodate variable batch sizes for train, val, test, and inference \n",
    "labels = tf.placeholder(tf.float32, [None, numClasses]) \n",
    "input_data = tf.placeholder(tf.int32, [None, maxSeqLength])\n",
    "# Using placeholder for dropout so we can have no dropout outside of training\n",
    "dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HKacPFqoa7Pd"
   },
   "source": [
    "Once we have our input data placeholder, we’re going to call the tf.nn.lookup() function in order to get our word vectors. The call to that function will return a 3-D Tensor of dimensionality batch size by max sequence length by word vector dimensions. In order to visualize this 3-D tensor, you can simply think of each data point in the integerized input tensor as the corresponding D dimensional vector that it refers to. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oPzVkaAva7Pe"
   },
   "source": [
    "![caption](https://github.com/honto-ming-hs/LSTM-Sentiment-Analysis/blob/master/Images/SentimentAnalysis13.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "CJX1nc-Ea7Pe"
   },
   "outputs": [],
   "source": [
    "# apparently embedding_lookup cannot be performed on GPU\n",
    "with tf.device('/cpu:0'):\n",
    "    W = tf.Variable(tf.constant(0.0, shape=[doc_vocab_size, embedding_dim]), trainable=True, name=\"W_embeddings\")\n",
    "    embedding_placeholder = tf.placeholder(tf.float32, [doc_vocab_size, embedding_dim])\n",
    "    embedding_init = W.assign(embedding_placeholder)\n",
    "    data = tf.nn.embedding_lookup(W,input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we want to use this graph for training with mini batch sets, and with validation and testing sets, the batch size will be variable, and will need to be fed to to Tensorflow at the time of graph execution. Therefor, we need to have a placeholder for the batch size so that we can provide it when we execute the graph for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = tf.placeholder(tf.int32, [], name='batch_size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NUG2kQ92a7Pg"
   },
   "source": [
    "Now that we have the data in the format that we want, let’s look at how we can feed this input into an LSTM network. We’re going to call the tf.nn.rnn_cell.BasicLSTMCell function. This function takes in an integer for the number of LSTM units that we want. This is one of the hyperparameters that will take some tuning to figure out the optimal value. We’ll then wrap that LSTM cell in a dropout layer to help prevent the network from overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZPSAyg8ca7Pg"
   },
   "source": [
    "Finally, we’ll feed both the LSTM cell and the 3-D tensor full of input data into a function called tf.nn.dynamic_rnn. This function is in charge of unrolling the whole network and creating a pathway for the data to flow through the RNN graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "B12_PKO7a7Pg"
   },
   "outputs": [],
   "source": [
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=dropout_keep_prob)\n",
    "init_state = lstmCell.zero_state(batch_size, dtype=tf.float32)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, initial_state=init_state, dtype=tf.float32, time_major=False)\n",
    "\n",
    "# improvements here can include passing sequence length so for reviews of length < 250 so that we capture the\n",
    "# LSTM state for the last word rather than the padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mbVfFb3ma7Pi"
   },
   "source": [
    "As a side note, another more advanced network architecture choice is to stack multiple LSTM cells on top of each other. This is where the final hidden state vector of the first LSTM feeds into the second. Stacking these cells is a great way to help the model retain more long term dependence information, but also introduces more parameters into the model, thus possibly increasing the training time, the need for additional training examples, and the chance of overfitting. For more information on how you can add stacked LSTMs to your model, check out Tensorflow's excellent [documentation](https://www.tensorflow.org/tutorials/recurrent#stacking_multiple_lstms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wgXY7-j0a7Pi"
   },
   "source": [
    "The first output of the dynamic RNN function can be thought of as the last hidden state vector. This vector will be reshaped and then multiplied by a final weight matrix and a bias term to obtain the final output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "TaZoQf8Qa7Pi"
   },
   "outputs": [],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DW264BBRa7Pk"
   },
   "source": [
    "Next, we’ll define correct prediction and accuracy metrics to track how the network is doing. The correct prediction formulation works by looking at the index of the maximum value of the 2 output values, and then seeing whether it matches with the training labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Z_FfZVQta7Pk"
   },
   "outputs": [],
   "source": [
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TL9hjsNKa7Pm"
   },
   "source": [
    "We’ll define a standard cross entropy loss with a softmax layer put on top of the final prediction values. For the optimizer, we’ll use Adam and the default learning rate of .001. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "m4CSEZOVa7Pn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tf_py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aaLpaxdha7Pp"
   },
   "source": [
    "If you’d like to use Tensorboard to visualize the loss and accuracy values, you can also run and the modify the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "5h1c00qka7Pp"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir + \"train/\", sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h0keo7xxa7Pq"
   },
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fyD99908a7Pq"
   },
   "source": [
    "Choosing the right values for your hyperparameters is a crucial part of training deep neural networks effectively. You'll find that your training loss curves can vary with your choice of optimizer (Adam, Adadelta, SGD, etc), learning rate, and network architecture. With RNNs and LSTMs in particular, some other important factors include the number of LSTM units and the size of the word vectors.\n",
    "\n",
    "* Learning Rate: RNNs are infamous for being diffult to train because of the large number of time steps they have. Learning rate becomes extremely important since we don't want our weight values to fluctuate wildly as a result of a large learning rate, nor do we want a slow training process due to a low learning rate. The default value of 0.001 is a good place to start. You should increase this value if the training loss is changing very slowly, and decrease if the loss is unstable.  \n",
    "* Optimizer: There isn't a consensus choice among researchers, but Adam has been widely popular due to having the adaptive learning rate property (Keep in mind that optimal learning rates can differ with the choice of optimizer).\n",
    "* Number of LSTM units: This value is largely dependent on the average length of your input texts. While a greater number of units provides more expressibility for the model and allows the model to store more information for longer texts, the network will take longer to train and will be computationally expensive. \n",
    "* Word Vector Size: Dimensions for word vectors generally range from 50 to 300. A larger size means that the vector is able to encapsulate more information about the word, but you should also expect a more computationally expensive model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2aaGIsAwa7Pr"
   },
   "source": [
    "### Training a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4sNG4bsOa7Pr"
   },
   "source": [
    "We've defined the model as a Tensorflow grpah. Now we need to tell Tensorflow what training steps are necessary. Here is an oversimplified description of how to train _any_ machine learning model:\n",
    "\n",
    "1. Give the model a batch of training data and truth labels\n",
    "2. The model makes a prediction on the batch based on it's current model parameters\n",
    "3. The predictions are compared with the correct labels for the training batch. An loss measure is calculated based on the difference between predictions and the correct labels\n",
    "4. The model adjusts its parameters just a little bit to lower the lost\n",
    "5. Repeat until loss is acceptable or it cannot be lowered any more\n",
    "\n",
    "Steps 2-4 are actually defined previously in the compute graph (2) ``prediction``, 3) ``loss``, 4) ``optimizer``). Also notice that these steps form a dependancy (``prediction`` > ``loss`` > ``optimizer``) in the compute graph.\n",
    "The way Tensorflow works is that you have to first define the compute graph (model), and you then tell Tensorflow what parts of the graph to execute in a Tensorflow session. Luckily, rather than listing all the parts of the graph to execute, you just have to identify the part furthest down the dependancy chain. Tensorflow will then execute all upstream parts autoamtically. In our case, our training step ends at the ``optimizer`` operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfhTft67E7Ko"
   },
   "source": [
    "To put it together,  we first define a Tensorflow session. Then, we load in a batch of reviews and their associated labels. Next, we call the `session.run()` function. This function has two arguments. The first is called the \"fetches\" argument. It defines the value we’re interested in computing/the part of the graph we want to execute. For our graph, we want the ``optimizer`` to be computed since that is the component that minimizes our loss function. The second argument is where we input our `feed_dict`. This data structure is where we provide inputs to all of our placeholders. We need to feed our training mini-batch of reviews and our batch of labels. This loop is then repeated for a set number of training iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SPJzm8cea7Pr"
   },
   "source": [
    "Instead of training the network in this notebook (which will take at least a couple of hours), we’ll load in a pretrained model (see next sub-section).\n",
    "\n",
    "If you decide to train this notebook on your own machine, note that you can track its progress using [TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard). While the following cell is running, use your terminal to enter the directory that contains this notebook, enter `tensorboard --logdir=tensorboard`, and visit http://localhost:6006/ with a browser to keep an eye on your training progress. **Note: I have not been able to get this on Google Colaboratory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ZBux5_D6a7Ps"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy for batch #0: 58.33333134651184\n",
      "Validation accuracy for batch #0: 50.24999976158142\n",
      "Train accuracy for batch #2000: 84.375\n",
      "Validation accuracy for batch #2000: 77.10000276565552\n",
      "Train accuracy for batch #4000: 92.70833134651184\n",
      "Validation accuracy for batch #4000: 76.8999993801117\n",
      "Train accuracy for batch #6000: 78.125\n",
      "Validation accuracy for batch #6000: 73.4499990940094\n",
      "Train accuracy for batch #8000: 92.70833134651184\n",
      "Validation accuracy for batch #8000: 80.54999709129333\n",
      "saved to models/pretrained_lstm.ckpt-10000\n",
      "Train accuracy for batch #10000: 88.54166865348816\n",
      "Validation accuracy for batch #10000: 69.9999988079071\n",
      "Train accuracy for batch #12000: 100.0\n",
      "Validation accuracy for batch #12000: 79.19999957084656\n",
      "Train accuracy for batch #14000: 98.95833134651184\n",
      "Validation accuracy for batch #14000: 79.29999828338623\n",
      "Train accuracy for batch #16000: 100.0\n",
      "Validation accuracy for batch #16000: 79.60000038146973\n",
      "Train accuracy for batch #18000: 100.0\n",
      "Validation accuracy for batch #18000: 77.60000228881836\n",
      "saved to models/pretrained_lstm.ckpt-20000\n",
      "Train accuracy for batch #20000: 98.95833134651184\n",
      "Validation accuracy for batch #20000: 78.39999794960022\n",
      "Train accuracy for batch #22000: 100.0\n",
      "Validation accuracy for batch #22000: 76.05000138282776\n",
      "Train accuracy for batch #24000: 100.0\n",
      "Validation accuracy for batch #24000: 76.05000138282776\n",
      "Train accuracy for batch #26000: 100.0\n",
      "Validation accuracy for batch #26000: 76.5500009059906\n",
      "Train accuracy for batch #28000: 100.0\n",
      "Validation accuracy for batch #28000: 77.64999866485596\n",
      "saved to models/pretrained_lstm.ckpt-30000\n",
      "Train accuracy for batch #30000: 100.0\n",
      "Validation accuracy for batch #30000: 77.70000100135803\n",
      "Train accuracy for batch #32000: 100.0\n",
      "Validation accuracy for batch #32000: 74.50000047683716\n",
      "Train accuracy for batch #34000: 100.0\n",
      "Validation accuracy for batch #34000: 74.09999966621399\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "#sess = tf.InteractiveSession()\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(embedding_init, feed_dict={embedding_placeholder: wordVectors})\n",
    "\n",
    "\n",
    "for i in range(iterations):\n",
    "    #Next Batch of reviews\n",
    "    nextBatch, nextBatchLabels = getTrainBatch(batchSize);\n",
    "    # feed dict for feeding data:\n",
    "    feed_dict = {input_data: nextBatch,\n",
    "                 labels: nextBatchLabels,\n",
    "                 dropout_keep_prob: 0.75,\n",
    "                 batch_size: batchSize\n",
    "                 }\n",
    "    sess.run(optimizer, feed_dict)\n",
    "    \n",
    "    #Write summary to Tensorboard\n",
    "    if (i % 50 == 0):\n",
    "        summary = sess.run(merged, feed_dict)\n",
    "        writer.add_summary(summary, i)\n",
    "\n",
    "    #Save the network every 10,000 training iterations\n",
    "    if (i % 10000 == 0 and i != 0):\n",
    "       save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "       print(\"saved to %s\" % save_path)\n",
    "        \n",
    "    # print the training and validation accuracy every 2000 iterations\n",
    "    if(i % 2000 == 0):\n",
    "        print(\"Train accuracy for batch #{}: {}\". \\\n",
    "              format(i, (sess.run(accuracy, feed_dict)) * 100))\n",
    "        valSet, valSetLabels = getValidationSet()\n",
    "        feed_dict_eval = {input_data: valSet,\n",
    "                          labels: valSetLabels,\n",
    "                          dropout_keep_prob: 1.00,\n",
    "                          batch_size: 2000\n",
    "                         }\n",
    "        print(\"Validation accuracy for batch #{}: {}\". \\\n",
    "              format(i, (sess.run(accuracy, feed_dict_eval)) * 100))\n",
    "        \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "77vYTYFca7Pt"
   },
   "source": [
    "I have saved the output from Tensorboard which shows the model’s accuracy and loss curves during training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZmpcCGbQa7Pu"
   },
   "source": [
    "![caption](https://github.com/honto-ming-hs/LSTM-Sentiment-Analysis/blob/master/Images/SentimentAnalysis6.png?raw=true)\n",
    "![caption](https://github.com/honto-ming-hs/LSTM-Sentiment-Analysis/blob/master/Images/SentimentAnalysis7.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y-Cy7z_Ja7Pu"
   },
   "source": [
    "The above diagrams are good visualization a that describes the training processs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LHyj0D2_a7Pt"
   },
   "source": [
    "#### Loading a Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vkNPLsEya7Pu"
   },
   "source": [
    "Any trained machine learning models can be thought of as consisting of 2 components:\n",
    "\n",
    "1. The architecture (computations & data structure) of the model\n",
    "2. The parameters for each computation for the model\n",
    "\n",
    "Notice the training data **is not part of a trained model**. The training of a machine learning model can be thought of as iteratively optimizing the parameters of the model based on data provided. When you ask Tensorflow to save a model, it basically saves these 2 components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a pretrained model involves defining another Tensorflow session, creating a Saver object, and then using that object to call the restore function. This function takes into 2 arguments, one for the current session, and one for the name of the saved model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "5QpZPs_ia7Pv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/pretrained_lstm.ckpt-20000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"models/pretrained_lstm.ckpt-20000\")\n",
    "# saver.restore(sess, tf.train.latest_checkpoint('models'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the training curves above, it seems that the model's training is going well. The loss is decreasing steadily, and the accuracy is approaching 100 percent. However, when analyzing training curves, we should also pay special attention to the possibility of our model overfitting the training dataset. Overfitting is a common phenomenon in machine learning where a model becomes so fit to the training data that it loses the ability to generalize to the test set. This means that training a network until you achieve 0 training loss might not be the best way to get an accurate model that performs well on data it has never seen before. That is one reason we use a validation set. The basic idea is that we train the model on our training set, while also measuring its performance on the validation set every now and again. Once the validation error stops its steady decrease and begins to increase instead (or alternatively the validation accuracy stops its steady increase and begins to decrease as you see around batch 16000), you'll know to stop training, since this is a sign that the network has begun to overfit. \n",
    "\n",
    "The validation set serves other purposes too. If we were comparing multiple different models, we will apply the same train-train-validate loop to each models, and compare their best validation accuracy/error to pick the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8x8Q9GSCa7Pw"
   },
   "source": [
    "Then we’ll load the movie reviews from our test set. Remember, these are reviews that the model has not been trained on and has never seen before. The accuracy for the test set can be seen when you run the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Jcusl1Zla7Pw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 78.7000000477\n"
     ]
    }
   ],
   "source": [
    "testSet, testSetLabels = getTestSet()\n",
    "print(\"Test accuracy\", (sess.run(accuracy, {input_data: testSet, \n",
    "                                            labels: testSetLabels,\n",
    "                                            dropout_keep_prob: 1.00,\n",
    "                                            batch_size: 2000})) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means if we released our model out in the wild, we'd expect it to be ~78% accurate in predicting whether the move review was positive or negative. Is that.. good?\n",
    "\n",
    "As with anything else, the answer is not simple. First, we should compare this to a baseline. Assuming that our dataset is a true representation of all movie reviews in that roughly half of all movie reviews are negative and half are positive, we can create a \"model\" that just predicts the review as positive, and that would give us 50% accuracy. A \"model\" that just randomly predicts positive or negative woudl also give an expected 50% accuracy. So we can set that as our baseline. \n",
    "\n",
    "We may also want to consider the types of mistakes our models make and how bad the impact is for each type of mistake. We are only predicting move reviews here so if 22% of our predictions are incorrect, it isn't too big of a deal. What if our model predicted whether a prisoner would re-offend, and the predictions are used by members of a parole board when considering parole applications? 78% accuracy may not be enough. You would also consider what proportion of the 22% incorrect predictions incorrectly predicts re-offend when in actuality the prisoner did NOT re-offend or incorrectly predicts NOT re-offend when in actuality the prisoner did re-offend. What is worst?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrate, Deploy, and Monitor your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a trained model with acceptable accuracy, you can integrate it as of the product your customers will be using. In ML lingo, the model is now used for _inference_ (fancy way of saying \"making predictions\"). We will not cover any of the details of how to integrate, deploy and monitor your model here. We will however, highlight the following:\n",
    "\n",
    "* There are tools you can use that takes a trained Tensorflow and other ML models and exposes them as a service. This includes [Tensorflow Serving](https://www.tensorflow.org/serving/), [Clipper.ai](http://clipper.ai/), and [AWS Sagemaker](https://aws.amazon.com/sagemaker/) \n",
    "* It is important to include mechanisms to monitor the accuracy and performance of your model over time. ML models suffer from [concept drift](https://machinelearningmastery.com/gentle-introduction-concept-drift-machine-learning/) which basically states that data changes over time so your model's accuracy will also degrade over time since it was trained on historic data.\n",
    "    * You will need to address the concept drift\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OC9-XM22a7Py"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hv5HtKMXa7Py"
   },
   "source": [
    "In this notebook, we went over the typical steps when attempting to solve a problem using machine learning. In particular, we cover a deep learning approach to sentiment analysis. We looked at the different components involved in the whole pipeline and then looked at the process of writing Tensorflow code to implement the model in practice. Finally, we trained and tested the model so that it is able to classify movie reviews.\n",
    "\n",
    "With the help of Tensorflow, you can create your own sentiment classifiers to understand the large amounts of natural language in the world, and use the results to form actionable insights. Thanks for reading and following along!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "LSTM_sentiment_analysis.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
